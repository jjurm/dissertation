This chapter consists of understanding the problem from the mathematical and research point of view.
I explain in detail why measuring robustness of graph metrics is an important and novel field, describing the work done in The Paper and similar academic studies.
Then I introduce definitions of graph metrics and robustness measures used in the implementation.


\section{Graphs and metrics}

Ever-increasing applications of data force researchers to use tools such as graph theory to model real-world problems, involving fields such as engineering, biology, chemistry, social systems, and many more~\cite{FouldsGraphTheoryApplications2012}.
Graph theory proved especially useful when analysing large structures of data which cannot simply be visualised.
Graph \textsl{statistics} (e.g. size, diameter, radius) help describe properties of arbitrarily large graphs.
Further, graph \textsl{metrics} (betweenness centrality, closeness centrality, eccentricity, etc.) quantify properties of individual nodes.


\section{Introduction to confidence}

Error measures are important in assessing how precise an observation is~\cite{Curran-EverettExplorationsStatisticsStandard2008}.
\enquote{A reported value whose accuracy is entirely unknown is worthless}~\cite{EisenhartExpressionUncertaintiesFinal1968}.
Confidence scores, confidence intervals, and error ranges signal or approximate the extent to which the observed result is reliable.

Networks obtained from the real-world may contain errors too, such as missing components, false positives, uncertain nodes or edges, and falsely aggregated nodes.
Network errors may stem from measurement errors, approximations, lack of knowledge, lack of experimental evidence or thresholding of scored networks~\cite{Wang2012,MarsdenNetworkDataMeasurement1990,JonesChallengesLimitationsQuantifying2010}.
These errors may then propagate to errors in graph metrics.

Can we measure precision or confidence of graph metrics?
If an outcome is based on evaluating metrics on graphs which are themselves imprecise or possibly have errors in the structure, how can we assess the reliability of such result?
Helping to answer these questions is the motivation behind building \graffs.

A number of attempts already tried to assess robustness or stability properties of graph metrics.
Examples are: analysing impact of errors on centrality measures in random networks~\cite{BorgattiRobustnessCentralityMeasures2006} or, more specifically, evaluating reproducibility of metrics on multi-temporal scans of human brain using coefficient of variation (CV), the repeatability coefficient (RC) and the intra class correlation (ICC)~\cite{VaessenEffectReproducibilityDifferent2010,DennisTestRetestReliabilityGraph2012}.
A significant amount of research analysing reproducibility of metrics in uncertain brain networks appeared around 2011, summarised in~\cite{TelesfordExplorationGraphMetric2013}.
The latest research led to development of methods for analysis of such uncertain data, estimating a ground-truth network structure from imperfect data~\cite{Martin2016,Newman2018}, and even a method to mitigate sensitivity of metrics on the selection of a threshold in scored networks~\cite{Drakesmith2015}.


\section{The Paper}

This dissertation project builds on top of the ideas from The Paper by Bozhilova et al.~\cite{Bozhilova2019}, a recent research on metric robustness.
The study, with main concepts explained below, proposes ways to quantify robustness of graph metrics in thresholded protein networks.

\subsection{Protein networks}

Protein interaction networks (PINs) are an example of a prominent research field relying on graph theory.
Nodes represent proteins present in cells of organisms and edges represent interactions between proteins.
Studying them helps in understanding physiology of biological cells and developing drugs.\footnote{For example, highly connected proteins and hubs of proteins in a cell are shown to be most essential for its survival~\cite{JeongLethalityCentralityProtein2001,HeWhyHubsTend2006}.}

Protein networks have a specific structure: nodes represent proteins and weighted (or scored) edges represent the presence of interactions between proteins.
It is assumed that such interactions are mutual, therefore the formed graph is undirected.
\cref{fig:ecoli_giant_at_900} shows an example of a protein network.

\input{figures/protein_network_schematic1.tex}
\input{figures/string_network_image_and_ecoli_giant.tex}

The weight of each edge in protein networks represents a confidence score which may be a value, or a set of values, describing some sort of \textsl{likelihood} that the interaction is true, given the available evidence (see \cref{fig:string_network_image}).
\Cref{sec:scored_networks} explains scored networks in detail.

\subsection{Network thresholding}

When researching protein interactions, in practice, a \textbf{threshold} is chosen and then a protein interaction network is formed from the database considering only interactions (edges) whose confidence score is greater than the selected threshold, to only take into account interactions of sufficiently high quality.
Thresholding edges of sufficient confidence is a common and necessary step before further graph analysis.

\subsection{Metric robustness}

As The Paper remarks, metric evaluation can be susceptible to the choice of the threshold.
However, a useful metric should lead to similar results across networks obtained at different reasonable confidence score thresholds.
Hence, the point of studying metric robustness, or stability, is a way to assess how reliable the results are on a graph derived from a given threshold.

When measuring robustness of node metrics, numerical values of metrics will undoubtedly depend on the chosen threshold.\footnote{For example, the degree and the clustering coefficient of each node will monotonically decline with declining density of the network, which monotonically declines as we increase the confidence threshold.}
For this reason, The Paper assesses robustness based on the \textbf{rankings} of nodes induced by each metric.

\subsubsection*{Rank robustness}

The Paper introduces three measures assessing robustness of node metrics (defined later):
\begin{itemize}[topsep=5pt]
    \item rank continuity (comparing highly ranked nodes at similar thresholds),
    \item rank identifiability (comparing ranks at various thresholds and the overall ranking),
    \item rank instability (quantifying variation of ranks of top 1\% of nodes).
\end{itemize}
All three measures are based on rankings of nodes induced by node metrics, and not on the exact values, to mitigate the density bias described above.

The Paper studies robustness of 25 different \textsl{node} metrics only, i.e. graph metrics which return one numerical value per node.


\section{Mathematical background}\label{sec:math_background}

In this section I review background material on graph theory (based mostly on~\cite{Estrada2017}) and fix terms to later avoid ambiguity.
Later, I introduce graph metrics, ranking, and graph metric robustness.

\input{figures_gen/simple_graph.tex}

Graphs (or networks) from graph theory consist of \textit{nodes} (or vertices, components) and \textit{edges} between them (links).
Formally, let \textbf{graph} $G$ be a pair $(V, E)$ of a finite set of $n$ \textbf{nodes} $V$ and a set of \textbf{edges} $E \subseteq V \times V$.
In this work I also use standard definitions of graph properties, relations on graphs, node adjacency and connectedness~\cite{Estrada2017}.
Those are omitted here for brevity, but included for reference in \cref{ch:math_definitions}.

Graphs I use in this project are all \textsl{simple graphs} (unless stated otherwise), i.e. edges are undirected (or bi-directional) and without loops.
\autoref{fig:simple_graph} is an illustration of a connected simple graph with 110 nodes and 151 edges, one of those I used for testing purposes when developing \graffs.

I may interchange the words \textsl{graph}, \textsl{network} and \textsl{dataset}, but generally: \textsl{dataset} is data stored in its raw form, such as a file downloaded from the Internet; \textsl{network} is the real-world graph-like data conveyed by a dataset; and \textsl{graph} is a hypothetical structure, a mathematical representation, and an interface for algorithms.

\input{figures_gen/disconnecting_graph.tex}

Most of real-world networks (such as social networks, citation networks or even protein interaction networks) contain a single giant component, and some small isolated components.
For example, in \autoref{fig:disconnecting_graph}, the red nodes form a giant component of the graph.

\subsection{Graph metrics}

Metrics are essentially functions of graphs that assign a value to each node.
They are used to quantify various properties of graphs, identify important nodes in different contexts, describe graph structures, and more.

Mathematically speaking, if $\mathbb{G}$ is the set of all graphs $G = (V, E)$ with $V \subset \mathbb{V}$, then we can define the domain of metrics to be generally functions of graphs:
\begin{equation}
    \label{eq:metric_type_def}
    \mathbb{M} \eqdef \mathbb{G} \Rightarrow (\mathbb{V} \Rightarrow \mathbb{R})
\end{equation}

Evaluation of a metric on a graph then results in a \textsl{mapping} of nodes to real numbers.
Therefore, for some metric $M \in \mathbb{M}$, some graph $G = (V, E)$, and a node $v \in V$: \footnote{However, for convenience, in the notation the graph argument is often left out when it is understood which graph the metric is applied to, resulting in $M(v) \in \mathbb{R}$.}
\begin{align}
    &M(G) : \mathbb{V} \rightarrow \mathbb{R}\\
    &M(G)(v) \in \mathbb{R}
\end{align}

The metrics introduced below are those I chose to implement in the \graffs tool.
Their definitions are based on The Paper~\cite{Bozhilova2019} (preferably, so that results can be reproduced), and on a paper summarising graph metrics~\cite{MartinHernandez2011}.
They are all defined for \textsl{undirected} graphs only.

\subsubsection*{Centralities}

One of the simplest metrics calculates the \textsl{degree} of each node, often considered as a centrality measure, indicating that nodes with higher degree are more important in the network.
\begin{flalign}
    \qquad\text{Degree:}\quad DC(v) \eqdef \deg(v) = \left\lvert \Set{v' \in V}{(v', v) \in E} \right\rvert
\end{flalign}

\textsl{Betweenness centrality} is calculated for each node $v$ as the number of all shortest paths passing through that node, for all pairs of nodes.
If there are multiple shortest paths between a pair of nodes, the fraction of those passing through $v$ are considered.
\begin{equation}
    \text{Betweenness:}\quad B(v) \eqdef \sum_{\substack{s,t \in V \\ s \ne v \ne t}} \frac{ \sigma_{st}(v) }{ \sigma_{st} }\,,
\end{equation}
where $\sigma_{st}$ is the number of shortest paths between $s$ and $t$, and $\sigma_{st}(v)$ is the number of those that pass through $v$.

\textsl{Closeness centrality} of a node $v$ is the reciprocal value of the sum of $d(v, i)$, i.e., the distances from that node to all other nodes $i$ in the graph.
It measures the reciprocal of the \textsl{farness} of each node to other nodes.
\begin{equation}
    \label{eqn:closeness_centrality}
    \text{Closeness:}\quad CC(v) \eqdef \frac{1}{\sum_{i \neq v} d(v, i)}
\end{equation}

This is sound for connected graphs, however, $d(v, i)$ is undefined if $v, i$ belong to two different components of the graph.
For disconnected graphs, I set $d(v, i) = \left\lvert V \right\rvert$, following the approach in The Paper.

\input{figures_gen/simplegraph_by_some_metrics.tex}

\textsl{Harmonic centrality} is a similar measure of \textsl{farness}.
\begin{equation}
    \label{eqn:harmonic_centrality}
    \text{Harmonic:}\quad HC(v) \eqdef \sum_{i \neq v} \frac{1}{d(v, i)},
\end{equation}
with $1 / d(v, i) = 0$ for disconnected nodes $v, i$ (as in~\cite{MarchioriHarmonySmallworld2000}).

\begin{wrapfigure}[11]{R}{0.23\linewidth}
    \input{figures/ego_network.tex}
\end{wrapfigure}

Both \textsl{closeness} and \textsl{harmonic centrality} require the computation of distances between all pairs of nodes.
I used an implementation of the Floyd–Warshall algorithm~\cite{FloydAlgorithm97Shortest1962} of time complexity $O({\left\lvert V \right\rvert}^3)$, which came bundled in the graph library I used (\cref{sec:graphstream}).
A significant improvement could be achieved by running Dijkstra's algorithm~\cite{dijkstra1959note} with time complexity $O(\left\lvert E \right\rvert + \left\lvert V \right\rvert \log \left\lvert V \right\rvert )$ starting in each node, or even better using Seidel's APD algorithm~\cite{SeidelAllPairsShortestPathProblemUnweighted1995}, but that was not the scope of my work.

\subsubsection*{Ego network measures}

Ego networks are local subgraphs centered around a particular node in the network.
Step-$n$ ego network (also called ego-$n$ network) of a node $v$ is a subgraph including $v$ (the \textsl{ego} node), all nodes reachable from $v$ in at most $n$ hops (the \textsl{alter} nodes), and all edges among these (see \autoref{fig:ego_network}).
For simplicity, sets $\ego_n$ below are defined just as a subset of nodes $V$.

For a graph $G = (V, E)$, define inductively $\ego_n : V \rightarrow \mathcal{P}(V)$:
\begin{align}
    \text{ego-$\mathbf{n}$ network:} \nonumber \\
    \ego_0(v) &\eqdef \left\{\, v \, \right\} \\
    \forall n \in \mathbb{N}.\ \ego_{n+1} (v) &\eqdef \ego_n \cup \Set{v' \in V}{ \exists v'' \in \ego_n(v).\ (v', v'') \in E }
\end{align}

Ego metrics are node metrics whose value depends only on properties of local ego network(s).
I chose and implemented 2 interesting ego metrics used also in The Paper: ego-$\mathbf{1}$ edges, and ego-$\mathbf{2}$ nodes.

\begin{equation}
    \text{Ego1Edges:}\quad E\mathit{1}E (v) = \left\lvert \Set{(v', v'') \in E}{ v' \in \ego_1(v) \wedge v'' \in \ego_1(v) } \right\rvert
\end{equation}
\begin{equation}
    \text{Ego2Nodes:}\quad E\mathit{2}N (v) = \left\lvert \ego_2(v) \right\rvert
\end{equation}
%    \begin{definition}[ego-$\mathbf{1/2}$ nodes ratio]
%        \vspace*{-5mm}
%        \[ ER(v) = \frac{ \left\lvert \ego_1(v) \right\rvert }{ \left\lvert \ego_2(v) \right\rvert } \]
%    \end{definition}

\textsl{Local clustering} measures the proportion of existing and all possible edges among neighbours of $v$, defined as in~\cite{WattsCollectiveDynamicsSmallworld1998}.
Following that, \textsl{redundancy} node metric is defined as the extent that $v$'s neighbours are adjacent to each other as well, according to~\cite{borgatti1997structural}.

Let $k = \deg(v)$, the degree of $v$, then
\begin{multline}
    \text{LocalClustering:}\nonumber \\
    LC(v) \eqdef
    \begin{dcases}
        0, &\text{if } k = 0,1 \\
        \frac{ \left\lvert \Set{ (v', v'') \in E }{ v' \in \ego_1(v) \wedge v'' \in \ego_1(v) } \right\rvert }{ k (k-1) / 2 } \quad &otherwise
    \end{dcases}
\end{multline}

\begin{equation}
    \text{Redundancy:}\quad R(v) \eqdef LC (v) \times (\deg(v) - 1)
\end{equation}
Note that for $k \leq 1$, $LC(v) = R(v) = 0$.

\subsubsection*{Page rank}

A different, interesting node metric is \textsl{PageRank}~\cite{BrinAnatomyLargescaleHypertextual1998}, first developed for Google Search to rate the importance of websites.
For brevity, the full definition is not included here.
I used an implementation of PageRank that is described in~\cite{ilprints422}.

%\subsubsection{Eigenvector centrality}
%\subsubsection{Katz centrality}
%- not used much recently
%\subsubsection{PageRank}
%= Katz centrality, but divide each vertex's contribution by its out-degree
%\subsubsection{Hyperlink-induced topic search (HITS)}
%Hubs and authorities, by Kleinberg
%\subsubsection{CommunityMeasure}
%- Modularity\\
%- Community Distribution\\
%- CommunityRelativeMeasure $\rightarrow$ NormalizedMutualInformation $\rightarrow$ VariationOfInformation
%\subsubsection{Eccentricity}
%\subsubsection{Surprise measure}

\subsection{Ranking}\label{sec:ranking}

It is clear that values of most metrics will naturally change with changing graph structure.
So to measure robustness of a metric on a higher level, The Paper defines robustness measures which only depend on \textbf{ranking} of nodes induced by given metric.
The important property of a stable metric is then to induce similar node rankings, not necessarily similar absolute values.

For example, in protein networks, the graph itself depends on the \textsl{confidence threshold} -- high thresholds lead to sparse graphs.
The degree of nodes will principally decrease with increasing threshold, however, degree happens to be a relatively stable metric in general, which means the sets of highest-degree nodes will be relatively similar across graphs of similar thresholds.

%\begin{definition}[Preorder]
%    Binary relation $\sqsubseteq$ on some set $P$ is a \textbf{preorder} iff it is reflexive and transitive, i.e.:
%    \begin{itemize}
%        \item $\forall a, b \in P.\ a \sqsubseteq a$ \tabto{7.3cm}(reflexivity)
%        \item $\forall a, b, c \in P.\ (a \sqsubseteq b \wedge b \sqsubseteq c) \implies a \sqsubseteq c$ \tabto{7.3cm}(transitivity)
%    \end{itemize}
%\end{definition}
%
%\begin{definition}[Total preorder]
%    A binary relation $\sqsubseteq$ on some set $P$ is a \textbf{total preorder} iff it is a preorder and a connex relation, i.e.:
%    \begin{itemize}
%        \item $\forall a, b, c \in P.\ (a \sqsubseteq b \wedge b \sqsubseteq c) \implies a \sqsubseteq c$ \tabto{7.3cm}(transitivity)
%        \item $\forall a, b \in P.\ a \sqsubseteq b \vee b \sqsubseteq a$ \tabto{7.3cm}(connexity, implies reflexivity)
%    \end{itemize}
%\end{definition}

Here I assume a standard definition of a total order (included in \cref{ch:math_definitions}).
A less-than-or-equal relation ($\leq$) on real numbers $\mathbb{R}$ is an example of a total order.

\begin{definition}[Ranking relation]
    \label{def:ranking_relation}
    A binary relation $\preceq$ on the set of nodes $V$ of some graph $G = (V, E)$ is a \textbf{ranking relation induced by a metric $M$ on a graph $G$} iff it is a total order satisfying:

    \[ \forall v_1, v_2 \in V.\ M(v_1) < M(v_2) \implies v_1 \preceq v_2 \]
\end{definition}

By convention, one can consider a ranking to be a bijection between nodes $V$ and the set $\integersto{\left\lvert V \right\rvert}$, with the associated integers being called \textbf{ranks}.
This definition means that ranking with respect to a metric is \textsl{one of possible} orderings of nodes such that nodes with high metric values correspond to high ranks.

\begin{definition}[Ranking (vector)]
    \label{def:ranking_vector}
    A \textbf{ranking vector} $A^{\preceq}$, or a \textbf{ranking}, induced by a metric $M$ on a graph $G$ is a bijection $V \leftrightarrow \integersto{\left\lvert V \right\rvert}$ such that
    \[ \forall v_1, v_2 \in V.\ v_1 \preceq v_2 \implies A^{\preceq}(v_1) \leq A^{\preceq}(v_2), \]
    where $\preceq$ is a ranking relation of graph $G = (V, E)$ with respect to metric $M$.

    Also, allow $A^{M(G)}$ as a shortened notation of a ranking induced by the metric $M$ on the graph $G$.
    Therefore,
    \[ A^{M(G)} : V \leftrightarrow \integersto{\left\lvert V \right\rvert} \]
\end{definition}

Note, there may be multiple valid rankings if multiple nodes have the same metric value, in which case any mutual order of such nodes is permitted in the ranking.
A ranking just assigns each node a rank (or a position) such that it does not violate the ordering by metric values.
Also, note that each node must be assigned a \textsl{unique} rank, due to the antisymmetry property.

\subsection{Robustness}

At the highest level where metrics are functions (\autoref{eq:metric_type_def}), robustness measures can be regarded as functions from $\mathbb{M}$ (the domain of all metrics) to real numbers.
\begin{equation}
    \mathcal{R} \eqdef \mathbb{M} \Rightarrow \mathbb{R}
\end{equation}

There are two relevant remarks to be made about robustness functions.

\paragraph*{Robustness is empirical.} The point of this dissertation is not to analyse the metrics purely as functions (which would indeed be difficult and not generalisable), instead, I analyse robustness \textbf{empirically}, by evaluating metrics on many graphs and deducing clues about how stable the values are across these multiple graphs.
A reader can find more on this is in the~\nameref{sec:methods} section.

\paragraph*{Per-graph robustness.} Robustness measures are calculated by evaluating metrics on graphs, and so values inherently depend on the graphs chosen (e.g. a metric may be more stable on dense graphs in general, where another metric may be unstable).
One could theoretically analyse a metric on a huge set of all kinds of graphs (such as social networks, road networks, internet networks, etc.), however, one can perhaps never construct a set covering all kinds of graphs.
The Paper calculates robustness by experiments on a few protein interaction networks (which presumably follow a similar structure), and then even dares to calculate the averaged robustness.

Per-graph robustness is, indeed, not helpful in answering the question ``Which metrics can I rely on when analysing my custom dataset X?'' which is a problem my project tackles.
Instead of coming up with more-general results, I defined \textbf{robustness} of a metric \textbf{on a certain graph} and rather built a \textsl{tool} that can perform this analysis generally on any given graph:
\begin{equation}
    \mathcal{R} \eqdef \mathbb{M} \times \mathbb{G} \Rightarrow \mathbb{R}
\end{equation}


\section{Methods}\label{sec:methods}

Taking previously defined concepts, here I introduce the specific methods that I used to implement the \graffs tool.
In particular, this section will discuss: datasets and their kinds, small perturbations on graphs, evaluating metrics, ranking nodes, and definitions of robustness measures.
Overall rank, $k$-similarity, and $\alpha$-relaxed $k$-similarity are the building blocks of the robustness measures, and will be defined throughout this section.

\subsection{Datasets}\label{sec:datasets}

In my work I distinguish two types of networks: scored and unscored.
Later in this section I present different publicly available graph datasets used in this project.

\subsubsection*{Scored networks}\label{sec:scored_networks}

In scored networks, edges are weighted, with each weight (also called \textbf{score}) representing the likelihood that the edge is true in the real world.
The score is usually a real value between $0$ and $1$, possibly scaled (see \cref{fig:graph_example_scored_edges}).
Scored networks naturally provide a way to introduce perturbations to the networks, by adjusting the confidence threshold.

Using a scored network as-is just by discarding the confidence score is usually unacceptable as many low-confidence edges would be false positives.
Obtaining a functional network from an almost fully connected clique needs thresholding at a reasonable value, and scored databases often state reasonable ranges of the confidence scores.

\input{figures_gen/graph_example_scored_edges.tex}

\paragraph*{STRING database {\normalfont\cite{Szklarczyk2019}}.} An open database of scored interactions between proteins.
At the time of writing, the database contains over 24M different proteins from over 5K organisms, accounting for over $2 * 10^9$ interactions.
It provides confidence scores for various types of evidence, such as scientific experiments, genetic observations, statistical predictions, text mining of scientific articles, or other existing knowledge.
An overall confidence score is calculated for each edge, conveying \enquote{the approximate probability that a predicted link exists between two enzymes (proteins)}.
The Paper, as well as my dissertation, use for simplicity only the overall confidence score, disregarding biological details.
The database offers the following guidance on the reasonable score values:
\begin{itemize}[topsep=5pt,itemsep=-2pt]
    \item low confidence -- 0.15 (or better)
    \item medium confidence -- 0.4
    \item high confidence -- 0.7
    \item highest confidence -- 0.9
\end{itemize}

\input{figures/table_scored_datasets.tex}
\input{figures/histogram_edges.tex}

A full-organism network is a subgraph of the huge database, containing only proteins of the given organism.
In my work, I evaluate robustness on 3 full-organism protein networks from the STRING database\footnote{Datasets are taken from the source files of The Paper's analysis instead of the STRING database directly, to match those in The Paper. Available at \url{https://github.com/lbozhilova/measuring_rank_robustness}}, listed in \cref{tab:scored_datasets}.
\Cref{fig:histogram_edges} shows the distribution of confidence scores in these networks.
Databases HitPredict~\cite{LopezHitPredictVersionComprehensive2015} and IntAct~\cite{OrchardMIntActProjectIntAct2014} are another examples of scored datasets, but not used in this project.

\subsubsection*{Unscored networks}\label{sec:unscored_datasets}

Unscored networks are unweighted graphs, not conveying any information about confidence of edges.
Thresholding a scored network and discarding the scores results in an unscored network.
There are numerous online sources providing interesting unscored network datasets.

\paragraph*{SNAP datasets.} Stanford Large Network Dataset Collection~\cite{Large2016} of the Stanford Network Analysis Project~\cite{LeskovecSNAPGeneralPurpose2016} provides open datasets obtained from real-world data such as social networks, citation networks, web graphs, internet networks, road networks and many more.

\paragraph*{KONECT datasets.} The Koblenz Network Collection~\cite{Kunegis2013} presents hundreds of network datasets of various types, covering areas such as \enquote{social networks, hyperlink networks, authorship networks, physical networks, interaction networks, and communication networks}.

I picked a few datasets (listed in~\cref{tab:unscored_datasets}) to demonstrate the concept of \graffs (see the~\nameref{ch:evaluation} chapter), considering mainly their supposed structure and size (number of nodes and density; due to associated computational cost).
Other huge online repositories such as the Network Repository~\cite{RossiNetworkDataRepository2015} provide access to thousands similar networks (outside of the scope of this work), which are also compatible with \graffs.

\input{figures/table_unscored_datasets.tex}

\subsection{Perturbing graphs}\label{sec:perturbing_graphs}

Many datasets are observations or approximations of real-world structures, not their exact representation (e.g. protein networks).
Analysing robustness of a metric means studying how different the metric values \textsl{could be} if we performed the same experiment on other datasets coming from the same source -- datasets possibly measured by different techniques, at different times, or in a different universe.

When studying a graph, there is often only one instance to work with, so to simulate having multiple similar\footnote{Here, ``similar'' means that they convey the same real-world structure.} graphs, we generate \textbf{perturbed graphs}, i.e. multiple samples with small perturbations.

\begin{savenotes}
    \begin{definition}[Graph generator]
        \vspace*{-2mm}
        A graph generator $\xi$ is a function that takes a graph $G$ and a number $n$\footnote{The exact value of $n$ is conceptually unimportant and is chosen individually in the~\nameref{ch:evaluation} chapter.} and generates a sequence of $n$ graphs based on $G$.
        \begin{equation}
            \mathlarger{\xi}(G) \generates{n} G_1, G_2, \dots, G_n
        \end{equation}
    \end{definition}
\end{savenotes}

\subsubsection*{Thresholding scored networks}

Various methods were proposed to choose a good baseline threshold, from fixing the value upfront~\cite{MeunierAgerelatedChangesModular2009} or after examining a range of values~\cite{vanWijkComparingBrainNetworks2010,HorstmannStateDependentProperties2010}, through maximising the threshold given some constraints~\cite{BassettAdaptiveReconfigurationFractal2006}, to choosing a value optimising some criterion such as classification rate~\cite{ZaninOptimizingFunctionalNetwork2012}.
Still, the confidence threshold value is a hyper-parameter of the graph-obtaining process and robust metrics should respond similarly to all reasonable values of the chosen threshold.

For scored networks, we generate perturbed graphs by considering different confidence thresholds (in a reasonable range) of the same scored network.
Resulting is a sequence of graphs with decreasing density as the threshold increases.
Define a \textbf{linear thresholding} graph generator, given a minimum threshold $\mu$ and a maximum threshold $\nu$, as follows:
\begin{align}
    &\mathlarger{^T \mathlarger{\xi}}_{\mu}^{\nu}(G) \generates{n} (V, E_i)~\text{for}~i\in \integersto{n}, \\[6pt]
    \text{where}~& E_i = \Set{(v_1, v_2)\in E}{ c(v_1, v_2) > \mu + \frac{i}{n - 1}(\nu - \mu) }, \nonumber \\
    &G = (V, E)~\text{is the base graph}, \nonumber \\
    &c(v_1, v_2)~\text{is the confidence score of the edge $v_1\,\text{---}\,v_2$} \nonumber
\end{align}

\input{figures_gen/thresholding_array.tex}

\subsubsection*{Randomly deleting edges}\label{sec:randomly_removing_edges}

For unscored networks, we generate a sequence of graphs by repeatedly deleting a random subset of edges.
One study~\cite{BorgattiRobustnessCentralityMeasures2006} proposes four kinds of random error in networks: deleting edges, deleting nodes, adding edges, adding nodes (and connecting them to the graph by random edges); all with a given small percentage of nodes or edges to be affected.

The two adding methods both depend on the particular score distribution for sampling new edges, which cannot easily be generalised to all graphs, hence I use random deletion.
Further, the 3 robustness measures I used are based on comparing sets of the highest ranked nodes, so deleting edges and preserving the set of nodes is more suitable than deleting nodes.\footnote{Moreover, deleting nodes could introduce more drastic and uncontrollable perturbations in dense networks, as deleting a high-degree node is a more extreme change than deleting a low-degree node. Thus, random node deletion would cause additional bias in these robustness measures.}

Define an \textbf{edge removing} graph generator, producing a sequence of graph, with a proportion of edges to delete $\delta$ (deletion rate) at each step:\footnote{\Cref{sec:validate_random_edges} shows how to choose a specific $\delta$}
\begin{align}
    &\mathlarger{^{ER} \mathlarger{\xi}}_\delta(G) \generates{n} G_1, G_2, \dots, G_n, \label{eq:edge_removing_generator}\\[6pt]
    \text{where}~& G_1 = G\,, \nonumber \\
    & \forall i \in \left\{2, \dots, n\right\}. \begin{array}[t]{l}
                                                    \forall e \in E.\ \chi_e\sim\mathcal{U}(0,1)\,, \\ G_i = \left( V_{i-1}, \Set{ e \in E_{i-1} }{ \chi_e > \delta }  \right)
    \end{array} \nonumber
\end{align}

Unlike linear thresholding, edge removing is applicable to unscored networks.
However, in practice, it is useful to specify an \textsl{initial threshold} so that this generator can work on scored networks, too, by turning them into unscored first.\footnote{Keeping all edges of scored networks and purely discarding the confidence scores would be detrimental to the graph structure, as low-scored edges would make scored networks appear falsely dense.}

\subsection{Evaluating metrics}\label{sec:evaluating_metrics}

We study how values of a metric change as graphs change with small perturbations.
Given a metric $M$, a base graph $G_0$, a graph generator $\xi$, and the number of perturbed graphs $n$, let \textbf{metric evaluation} refer to the application of a metric to a sequence of generated graphs:
\begin{align}
    M \overset{\mathlarger{\xi}}{\vdash} G_0 &\generates{n} M(G_1), M(G_2), \dots, M(G_n), \\[10pt]
    \text{with}\qquad \mathlarger{\xi}(G_0) &\generates{n} G_1, G_2, \dots, G_n \nonumber
\end{align}

Further, define \textsl{overall ranking} on a sequence of perturbed graphs that is calculated by first averaging node ranks over all induced rankings, and then ranking the averaged values.
This will be used in the following robustness definitions.

\begin{definition}[Overall ranking]
    \label{def:overall_ranking}
    An \textbf{overall ranking} $A^*$ of a metric $M$ on $n$ perturbed graphs generated using $\xi$ from a base graph $G_0$, is defined as:
    \begin{align*}
        &\mathlarger{A^*}\!\left( M \mid G_0, \xi, n \right) = A^{\operatorname{Avg}(G_0)} ,\\[5pt]
        \text{where}~&\operatorname{Avg}(G) :\ \forall v \in V.\ v \,\mapsto\, \frac{1}{n} \sum_{i=1}^n A^{M(G_i)}(v)\, , \\
        &\begin{aligned}
             \text{where}~\,&\xi(G) \generates{n} G_1, \dots, G_n\,, \\
             & G = (V, E)
        \end{aligned}
    \end{align*}
\end{definition}

In the definition, $A^{M(G_i)}$ is the ranking induced on one generated graph.
For each node $v$, we calculate the average of $v$'s rank across all rankings, and call it a ``metric'' $\operatorname{Avg}$.
Then the overall ranking is just the ranking of the locally defined $\operatorname{Avg}$ metric.

\subsection{Robustness measures}\label{sec:robustness_measures}

I define the 3 robustness measures: rank continuity, rank identifiability, and rank instability.
These were proposed in The Paper, however, the original definitions only work for linearly thresholded scored networks.
I generalised their definitions and extended them to unscored networks, too.

The Paper argues that in the context of bioinformatics, metrics are often used to identify key nodes of graphs, therefore it is natural to focus on the highest ranking nodes only.
This is also true for networks that are just too large for each node to be inspected individually.
Hence, in principle, these robustness measures test how reliably a metric can identify a certain number of highest-ranked nodes across perturbed graphs.

Let us first define $k$-similarity and $\alpha$-relaxed $k$-similarity as in The Paper.
Both similarities quantify the correlation between two rankings (similar to Spearman or Kendall rank correlation coefficients), but only take into account the highest-ranked nodes, thus suiting mainly applications where identification of highest-ranking nodes is important.

\begin{definition}[$\bm{k}$-similarity]
    \label{def:k_similarity}
    \vspace*{-2mm}
    The \textbf{$\bm{k}$-similarity} of two rankings $A_\theta, A_\mu$ is the overlap between their $100k\%$ highest ranking nodes:
    \[ \operatorname{Sim}_k(A_\theta, A_\mu) = \frac{\left\lvert \Set{v \in V^{\cap}}{ A_\theta(v) > N(1-k) \wedge A_\mu(v) > N(1-k) } \right\rvert}{Nk} \]
    where $k \in \left( 0, 1 \right]$ and $V^{\cap}$ is the intersection of domains of $A_\theta, A_\mu$.\\
    Note that this measure of rank similarity is symmetric.
\end{definition}

The following $\alpha$-relaxed $k$-similarity accounts for situations where the two rankings are to be interpreted differently (e.g. comparing a perturbed graph's ranking with an overall ranking).

\begin{definition}[$\bm{\alpha}$-relaxed $\bm{k}$-similarity]
    \label{def:alpha_relaxed_k_similarity}
    \vspace*{-2mm}
    The \textbf{$\bm{\alpha}$-relaxed $\bm{k}$-similarity} of two rankings $A_\theta, A$ is the proportion of the $100k\%$ highest ranking nodes in $A$ which are also within $100k\bm{a}\%$ highest ranking nodes in $A_\theta$:
    \[ \operatorname{Sim}_k^\alpha(A_\theta, A) = \frac{\left\lvert \Set{v \in V^{\cap}}{ A_\theta(v) > N(1-k\bm{\alpha}) \wedge A_\mu(v) > N(1-k) } \right\rvert}{Nk} \]
    where $\alpha>0$, $k, k\alpha \in \left( 0, 1 \right]$ and $V^{\cap}$ is the intersection of domains of $A_\theta, A$.
\end{definition}

For example, if $A_\theta$ is obtained by ranking a perturbed graph and $A$ is an overall ranking (of multiple perturbed graphs), relaxed $k$-similarity may be used to identify whether the top 10 nodes overall (in $G$), are among the top 15 for the particular observed threshold (in $G_\theta$).

Following are definitions of rank robustness measures, given a metric $M$, a base graph $G$, a graph generator $\xi$, and the number of perturbed graphs to generate $n$.

\textsl{Rank continuity} is defined as the proportion of pairs of consecutively generated graphs (e.g. thresholded at consecutive values), for which the $k$-similarity of node rankings is above $0.9$.

\newcommand*{\args}{\left(M \mid G, \xi, n \right)}
\newcommand*{\astar}{A^*\!\args}
\begin{definition}[Rank continuity]
    \label{def:rank_continuity}
    \vspace{-0.5cm}
    \begin{flalign}
        \begin{split}
            &\operatorname{RankContinuity} \args \eqdef \\
            &\qquad \frac{ \norm{\Set{(i, k) \in \integersto{n-1} \times \mathcal{K}}{ \operatorname{Sim}_k\left(A^{M(G_{\mathlarger{i}})}, A^{M(G_{\mathlarger{i+1}})} \right) \geq 0.9 }} }{ (n - 1)\norm{\mathcal{K}} }
        \end{split} \\[6pt]
        \text{where }\ & \mathcal{K} = \Setenum{ 0.001, 0.002, \dots, 0.05 }, \nonumber \\
        & \xi(G) \generates{n} G_1, G_{i+1}, \dots, G_{n}, \nonumber \\
        & \operatorname{Sim}_k(\dotp, \dotp)\ \ \text{is $k$-similarity of two rankings, by~\autoref{def:k_similarity}} \nonumber
    \end{flalign}
\end{definition}

Secondly, \textsl{rank identifiability} attempts to quantify, how well ranks induced by a metric on perturbed graphs resemble the overall ranking.
Taking $\alpha$-relaxed $k$-similarity values between ranks of perturbed graphs and an overall ranking (with respect to a metric), rank identifiability is defined as their minimum.
The role of the $\alpha$ coefficient is \enquote{to allow for more user control when the rankings compared are not interpreted in the same way}~\cite{Bozhilova2019}.

\begin{definition}[Rank identifiability]
    \label{def:rank_identifiability}
    \vspace{-0.5cm}
    \begin{flalign}
        &\operatorname{RankIdentifiability} \args \eqdef \min\limits_{i \in \integersto{n}} \operatorname{Sim}_k^\alpha(A^{M(G_{\mathlarger{i}})}, A^* ) \\[10pt]
        \text{where }\ & A^* \quad \text{is an overall ranking, by~\autoref{def:overall_ranking}}, \nonumber \\
        & \xi(G) \generates{n} G_1, G_{i+1}, \dots, G_{n}\,, \nonumber \\
        & k \approx 0.023478,\,\footnote{The Paper uses slightly different value of $k$ for each dataset, so in order to generalise this process I set $k$ to be a constant value calculated as the average of the values used in The Paper.} \nonumber \\
        & \alpha = 1.5, \nonumber \\
        & \operatorname{Sim}_k^\alpha(\dotp, \dotp)\ \ \text{is $\alpha$-relaxed $k$-similarity of two rankings, by~\autoref{def:k_similarity}} \nonumber
    \end{flalign}
\end{definition}

Finally \textsl{rank instability} measures how much the rank of each individual node changes.
$U$ is the set of the $1\%$ highest ranked nodes according to the overall ranking $A^*\!\left(M \mid G, \xi, n \right)$.
Then, for each node $v \in U$, we calculate a range of \textsl{ranks} that the node attains across the generated graphs.
Rank instability is then the average scaled range for the nodes in $U$.

\begin{definition}[Rank instability]
    \label{def:rank_instability}
    \vspace{-0.5cm}
    \begin{flalign}
        &\operatorname{RankInstability} \args \eqdef \frac{1}{\left\lvert U \right\rvert}\, \mathlarger{\sum}_{v \in U}\, \frac{\operatorname{range}(v)}{n} & \\[10pt]
        \text{where }\ & U = \Set{v \in V}{A^* (v) > 99\%n}, \nonumber \\[2pt]
        & \operatorname{range} : \begin{array}[t]{l}
                                     V \to \mathbb{R}\\ v \,\mapsto\,  \max\limits_{i \in \integersto{n}} \left( A^{M(G_{\mathlarger{i}})}(v) \right)  -  \min\limits_{i \in \integersto{n}} \left( A^{M(G_{\mathlarger{i}})}(v) \right)\ ,
        \end{array} \nonumber \\
        & A^* \quad \text{is an overall ranking, by~\autoref{def:overall_ranking}} , \nonumber \\
        & G = (V, E), \nonumber \\
        & \xi(G) \generates{n} G_1, G_{i+1}, \dots, G_{n} \nonumber
    \end{flalign}
\end{definition}

High values of RankContinuity and RankIdentifiability mean high robustness of a metric, whereas high values of RankInstability mean low robustness.

Apart from the robustness measures above, there are other approaches one could investigate.
For example, Borgatti et al.~\cite{BorgattiRobustnessCentralityMeasures2006} suggest \enquote{Number of nodes in both the top $10\%$ of the true network and of the observed network, divided by the number of nodes}, and \enquote{Square of the Pearson correlation between true centralities and observed centralities} as robustness measures -- these could be further investigated in future research, or easily embedded in \graffs, too.
