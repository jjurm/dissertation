This chapter describes first the overall structure of the project, its technical requirements and then dives into the implementation of individual modules of \graffs.


\section{Overview}

The purpose of this project is to develop a methodology and framework, i.e. a tool, to help study graph metrics, and empirically analyse their robustness in particular.

\graffs is a command-line tool written in Kotlin that can load/store datasets of different formats, generate perturbed graphs, evaluate metrics, and calculate robustness values. \autoref{fig:brief_pipeline} explains the natural flow of the program, i.e. the \textsl{main pipeline} starting with graph datasets and ending up with deductions about each metric's \textsl{robustness}.

\input{figures/brief_pipeline.tex}


\section{Design goals}\label{sec:design-goals}

The following are technical requirements I set for the project.
Overall, I aim this tool to be reusable for similar projects, either by directly invoking the compiled binary, or by using it as a dependency, or by forking and extending it.
The target user is a researcher or anyone who would benefit from assessing the stability of graph metrics evaluated on arbitrary graphs.

\subsection{Supported features}

\graffs supports the following features:

\begin{enumerate}[itemsep=-2pt,topsep=0pt]
    \item Load input graphs in various formats, and represent them in a unified memory structure
    \item Run algorithms that compute metrics on graphs
    \item Generate graphs by applying perturbations to given input graphs
    \item Run experiments by evaluating metrics on generated graphs in a systematic manner
    \item Calculate robustness of metrics based on experiments
    \item Possibly, produce visual output from the results
\end{enumerate}

\subsection{Scalability}

According to The Paper~\cite{Bozhilova2019}, calculating natural connectivity for a single node for a graph with ~7000 nodes takes ~88 seconds on a standard computer.
In one of my toy examples, calculating average betweenness centrality of ~2500 nodes took ~8 minutes on my personal computer.
Thus, assuming computing a (computation-heavy) metric(s) on an input graph of average size 5000 nodes takes ~30 minutes, the pure computation time suggested by the~\nameref{ch:proposal} would take the following time on a standard personal computer (approximated in the order of magnitude)
\[(\sim 6\ \text{datasets}) \times (\sim 6\ \text{metrics}) \times (\sim 50\ \text{derived graphs}) \times (\sim 30\ \text{minutes}) \approx 38\ \text{days}\]

For this reason, one of the goals is to make the program efficient and runnable on a supercomputer, utilising the power of high-performance \textbf{multi-core systems} for \textbf{parallel execution}.

\subsection{Reproducibility}\label{sec:reproducibility}

It is important for results and outcomes in this field to be reproducible.
Reproducibility of \graffs is the guarantee that, given the same input, one can exactly reproduce any results produced by the program including the same robustness values, images, tables, numbers up to a bit-wise match where applicable.
\graffs solves challenges the following areas:
\begin{itemize}
    \item \textbf{Stochastic processes}

    Methods based on stochastic processes or randomness must be reproducible.
    An example of a stochastic method are graph generators.

    \graffs makes these reproducible by generating all randomness starting off with a given seed for the generator.

    \item \textbf{Resolving ties}

    Methods that require pseudo-randomness to resolve ties must also be reproducible.
    An example is ranking nodes of a graph according to metric values, or generating a visual layout for graphs such as in \autoref{fig:simple_graph}.
    Again, a solution is to base such flow on an input seed.

    \item \textbf{External factors}

    Unpredictability introduced by external factors such as the machine, operating system, thread scheduling, time or other factors, must be accounted for.
    These issues are resolved using a robust programming language and by appropriately synchronising parts of the program, where this is relevant.
\end{itemize}

\subsection{Flexibility}

The program must be flexible enough, in particular the following:
\begin{enumerate}[topsep=5pt,itemsep=0pt]
    \item Usable on all widely used machines and operating systems
    \item Accepting input datasets (and any input parameters) in common formats, in particular those datasets mentioned in \cref{sec:datasets}
    \item As a library, it must provide modular access to individual parts of the program, so that it is easy to use \graffs as a dependency in future projects of a similar kind
\end{enumerate}


\section{Architecture}

I built \graffs in the Kotlin programming language, with the help of Git and Gradle, and here I explain why.

\subsection{Kotlin language}

\textbf{Kotlin}~\cite{JemerovKotlinAction2017} was chosen mainly for the following reasons.
\begin{itemize}[topsep=5pt]
    \item Safer, preventing a significant number of errors
    \item Concise, reducing the amount of boilerplate code
    \item IDE-friendly, allowing the IDE to help with software engineering
    \item Employs functional programming paradigms~\cite{Bonev}, abstracting high-level concepts
    \item Compiles to Java byte code and so preserves other important benefits of Java: Object-Oriented, multi-threaded, platform-independent, secure and easily extensible.
\end{itemize}

Kotlin is by nature similar to Java and can be used together with other Java code or libraries in a single project, as Kotlin compiler compiles \texttt{.kt} files to Java-bytecode \texttt{.class} files.
It adopted most concepts from Java, such as classes, polymorphism, inheritance, which I heavily use in the project.
Performance-wise, Kotlin is comparable to Java.

There is one notable feature of Kotlin: properties, which are an abstraction of fields and getters/setters in classes and help decouple the implementation of the class from its interface even more.
In UML diagrams below, properties are presented instead of fields.

\subsection{Build automation}

The project uses Gradle~\cite{BerglundBuildingTestingGradle2011} for project management.
Split into different modules, Gradle also helps to keep the structure well defined and manages builds of each module separately.

Project configuration rules are set up using the \texttt{build.gradle} files (one in the root directory, then one within each module) with a number of plugins to facilitate the following and more:
\begin{enumerate}[topsep=5pt,itemsep=0pt]
    \item Defines the structure of the project, such as source and build directories for each module
    \item Manages and automatically downloads dependencies
    \item Automates the process of code compilation, unit testing and producing deployable \texttt{jar}s
    \item Generates HTML API documentation from the \texttt{KDoc} language\footnote{Using the \texttt{Dokka} tool, similar to \texttt{JavaDoc}}
\end{enumerate}

Unit tests are written in the JUnit 5 testing framework (42 tests in total), making sure that the functionality of managing graphs and evaluating metrics does not diverge from what is expected.
\graffs can easily be build from sources by running |gradle build|, which downloads dependencies, compiles modules, runs tests and produces the |cli/build/libs/graffs-last.jar| file that has all necessary libraries bundled and can be run directly.


The commit hash, for example, is used to produce a commit-specific version number when packaging \graffs.

CircleCI, a continuous integration service, is triggered with each commit to test the build pipeline including tests.

\subsection{Licensing}

In this project I used libraries with different open-source licenses: LGPLv3 (the GraphStream library, described later), LGPLv2.1 (Hibernate ORM), APSL 2.0 (Clikt, Tablesaw, and some Apache libraries \texttt{commons-*}), MPLv2.0 (H2), GPLv2 or later (Renjin), MIT (java-express).

A working version of \graffs is published on GitHub along with its source code under the GPLv3 licence~\cite{gplv3} (open-source), which is in line with licences of used libraries.

\subsection{Computing cluster}\label{sec:computing_cluster}

The \graffs tool is programmed to allow parallel computing using multiple threads on a single machine.
Having experimented with a number of frameworks for high-performance computing, I chose the \texttt{kotlinx.coroutines} library~\cite{TorresLearningConcurrencyKotlin2018} (see~\autoref{sec:kotlin_coroutines}), to make \graffs portable and easy to set up.
It is also straightforward to manually run multiple simultaneous instances of \graffs at different machines with a shared database, with each instance contributing to a different part of the evaluation.\footnote{Off-the-shelf products such Apache Spark~\cite{ZahariaApacheSparkUnified2016}, Apache Storm, and Spring Boot have native support for computation across multiple machines, but are heavyweight and require user setup.}

I used two remote high-performance computing servers\footnote{sponsored by Dr Andrew Moore, from the System Research Group at the Computer Laboratory, University of Cambridge: \url{https://www.cl.cam.ac.uk/research/srg/}}:

\begin{description}[itemsep=-2pt]
    \item[\texttt{rio.cl.cam.ac.uk}] running \textsl{Ubuntu 18}, using \textsl{4x 8-Core AMD Opteron 6128} with 16 MiB L2 cache (32 cores in total), 128 GiB RAM
    \item[\texttt{nile.cl.cam.ac.uk}] running \textsl{Ubuntu 18}, using \textsl{2x 12-Core AMD Opteron 6168} with 12 MiB L2 cache (24 cores in total), 128 GiB RAM
\end{description}

\subsection{Project modules}

\input{figures/module_dependency_matrix.tex}

The project is structured in the following modules, using multi-project builds in Gradle:
\begin{itemize}[topsep=5pt]
    \item \texttt{api} - interfaces of structures, metrics, generators
    \item \texttt{db} - database data model
    \item \texttt{metrics} - graph metrics
    \item \texttt{robustness} - node ranking, rank similarities, robustness measures
    \item \texttt{graph} - graph generators, loading datasets from filesystem
    \item \texttt{cli} - command-line interface: creating and evaluating experiments, visualising graphs, generating rank similarity plots, etc.
    \item \texttt{figures} - code for automating generation of some figures in this report
\end{itemize}

\Cref{fig:module_dependency_matrix} shows inter-module dependencies.
An extended dependency matrix in \cref{fig:classes_dependency_matrix} in \cref{ch:appendix_kotlin_figures} contains individual project files.


\section{Data model}

The main concept is based on the three persisted entities:
\begin{itemize}[topsep=5pt]
    \item \textsl{Graph} storing its nodes, edges (and their attributes)
    \item \textsl{Graph generator} is an object capable of producing a number of graphs, given a dataset.
    \item \textsl{Experiment} is a description of a computational task involving:
    \begin{enumerate}[topsep=0pt,itemsep=0pt]
        \item generating graphs from given input datasets using a given graph generator
        \item evaluating a given set of metrics on all perturbed graphs
        \item evaluating given robustness measures on all metrics over all datasets
    \end{enumerate}
\end{itemize}

A user can create and manage \textsl{graph generators} and \textsl{experiments} using the command line interface (see \autoref{sec:cli}).
The \graffs tool uses a number of libraries to represent graphs in memory, define a data model, and store data in a relational database.

\subsection{Using GraphStream}\label{sec:graphstream}

Graphs in memory are stored and manipulated by the GraphStream library~\cite{DutotGraphStreamToolBridging2007}, a \enquote{Java library for the modeling and analysis of dynamic graphs}.\footnote{As alternatives, I also investigated and considered library JGraphT~\cite{Michail2019}, which is algorithm-focused, however, the algorithms relate mostly to walking on graph nodes and are not relevant for evaluating graph metrics.
GraphStream also provides visualisations as opposed to JGraphT.}
The library consists of 3 modules: \texttt{gs-core} defining the API and underlying structures, \texttt{gs-algo} with various algorithms from graph theory, and \texttt{gs-ui} containing components for visualising graphs.
Even though library allows working with dynamic graphs (changing over time), this project only uses static graphs.

The GraphStream library provides a solid base for numerous features in this project such as loading graph files, storing them in various formats, and visualisations.
The library also contains an implementation of some graph metrics.
In practice, the library's interfaces contain many more links and methods (e.g. for handling directed graphs) that are not relevant in the context of this project.

\input{figures_gen/graphstream_diagram.tex}

The library is based around the \texttt{Graph} interface, which provides access to \texttt{Node}s and \texttt{Edge}s of each graph.
The most relevant interfaces are illustrated in \autoref{fig:graphstream_diagram} (heavily simplified).
All \texttt{Element}s (i.e. \texttt{Node}s, \texttt{Edge}s and even \texttt{Graph}s) have an \texttt{id} field which will be used for matching corresponding nodes between base and perturbed graphs.
The interfaces also provide methods for changing graphs (adding/removing nodes, edges, attributes, etc.).

The \texttt{Graph} object from the GraphStream library stores (references to) all nodes and edges, along with their attributes.
Calculated metric values of each node, as well as some metadata, is stored as each \texttt{Elements}'s attribute, with the attribute key being the metric name.

\subsection{Relational model}

Data such as generated graphs, evaluated metrics, robustness measure results as well as any user-defined hyper-parameters are stored in a relational database system.
The Java Persistence API (JPA)~\cite{BiswasJavaPersistenceAPI2016} provides an abstraction for accessing relational data from Java, Hibernate~\cite{ElliottHibernateDeveloperNotebook2004,BauerJavaPersistenceHibernate2015} is a framework that implements the inteface.
I used specifically the H2 database engine~\cite{MuellerH2DatabaseEngine2006} as the underlying storage for Hibernate.
This abstraction is later illustrated in \autoref{fig:orm_kotlin_h2_diagram}.

\input{figures_gen/data_model_diagram.tex}

\autoref{fig:data_model_diagram} shows the entities that the program persists in the database, as explained below.
Named entities (\texttt{Experiment}, \texttt{GraphGenerator}) are those that the user creates and later refers to with their name.
\begin{description}[itemsep=\zerospace]
    \item[\texttt{PerturbedGraph}]
    stores a deflated serialised version of the \texttt{Graph} object, with its metadata (how it was generated), including all so-far evaluated metric values of nodes, and possibly a weight associated with each edge (for scored networks).

    \item[\texttt{GraphCollection}]
    represents an (ordered) collection of \texttt{PerturbedGraph}s, and keeps track of which datasets the graphs were generated from.

    \item[\texttt{GraphGenerator}]
    stores user-defined rules for generating graphs from a dataset.
    The parameters include the \texttt{method} to use (such as \texttt{linear-thresholding}), number \texttt{n} of perturbed graphs to generate from each input dataset, seed, and any additional numeric parameters specific to each generator.
    One graph generator can be used across multiple experiments.

    \item[\texttt{Experiment}]
    encapsulates a concept of evaluating multiple \textsl{robustness measures} of multiple \textsl{metrics} on multiple \textsl{datasets}, using a specific graph generator.

    \item[\texttt{Robustness}]
    stores a single result of evaluating a \textsl{robustness measure} of a \textsl{metric}, on a set of perturbed graphs originating from a certain \textsl{dataset}.
    Note that the four fields \texttt{experiment}, \texttt{dataset}, \texttt{metric}, \texttt{robustnessMeasure} together form the primary key (\autoref{fig:data_model_diagram}).
\end{description}

\subsection{Java Persistence API}

The Java Persistence API~\cite{BiswasJavaPersistenceAPI2016} (JPA) is an API specification for management of relational data in Java.
It describes ways in Java to specify schemas of relational databases and an interface to manage and access data of a relational model (i.e. entities in tables, relations, first-order logic).
\textsl{Persistence} is an abstract term referring to accessing, managing, and storing entities.

The Hibernate framework~\cite{ElliottHibernateDeveloperNotebook2004}, an object-relational mapping tool, provides a concrete implementation of JPA~\cite{BauerJavaPersistenceHibernate2015}.
I use Hibernate as the intermediate layer between the \texttt{core} module of \graffs and the underlying H2 database that is completely abstracted away from the \graffs code (\autoref{fig:orm_kotlin_h2_diagram}).

\input{figures/orm_kotlin_h2_diagram.tex}

Further, \autoref{fig:data_model_classes_diagram} shows the underlying \textsl{entity classes} written in Kotlin that have the following functions:
\begin{itemize}[topsep=5pt,label=$\bm{\rightarrow}$]
    \item \textbf{Data model definition}, seen in \autoref{fig:data_model_diagram}

    JPA provides a number of Java annotations to define entities (\texttt{@Entity}), their fields (\texttt{@Column}), constraints such as foreign key constraint (\texttt{@OneToMany}, \texttt{@ManyToOne} and others), and metadata such as rules for fetching data from database (e.g. \texttt{@Basic(fetch = FetchType.LAZY)} for lazy fetching of a field).
    Data model is inferred from these annotations during compile time.

    Hibernate then abstracts away also operations such as creating and updating the database schema.
    See \autoref{lst:kotlin_experiment_entity} for a full code listing of the \texttt{Experiment} entity using JPA annotations.

    \lstinputlisting[label={lst:kotlin_experiment_entity}, caption={The \texttt{Experiment} class written in Kotlin. Note especially the annotations which are enough to define the JPA data model.}, float, firstline=11,language=Kotlin]{Experiment.kt}

    \item \textbf{Metamodel generation}

    Taking the annotated entity classes during compile time, Hibernate generates a \textsl{metamodel} class for each entity to allow writing type-safe queries.
    For example, the \texttt{Experiment\_} class is generated from the \texttt{Experiment} entity, with corresponding fields (such as \texttt{Experiment\_.name} of type \texttt{SingularAttribute<NamedEntity, String>}, or \texttt{Experiment\_.generator} of type \texttt{SingularAttribute<Experiment, GraphGenerator>}).
    See \autoref{lst:jpa_typed_query} for an illustration how generated metamodel classes aid in writing type-safe queries.

    \item \textbf{Object-relational mapping}

    The classes (\autoref{fig:data_model_classes_diagram}) themselves carry the data managed by Hibernate.
    Other modules such as \texttt{robustness} and \texttt{cli} can use entity classes, while they are also managed by Hibernate.
    Hibernate is responsible for loading data into entity objects as well as persisting such objects.
    See \autoref{lst:hibernate_load_entity} for an illustration.
\end{itemize}

\subsection{H2 Database}

I employed the H2 relational database~\cite{MuellerH2DatabaseEngine2006} (based on SQL language) for storing entities, considering the following:
\begin{itemize}[topsep=5pt,itemsep=-2pt]
    \item Very fast; small footprint on the system in terms of installation complexity, program size, and cost of backround maintenance
    \item Easily embeddable, as it implements the JDBC API (Java Database Connectivity API, used by Hibernate, too)
    \item Supports in-memory databases (good for testing)
    \item Written purely in Java, so can be bundled in \graffs and thus requires no other database installation in the client OS
\end{itemize}
Graphs are stored in the database as deflated serialised \texttt{Graph} objects, including their \texttt{Node}s, \texttt{Edge}s and their attributes.
One an experiment is done, SQL queries such as \autoref{lst:sql_robustness} can be used to manually retrieve the data.

\lstinputlisting[label={lst:sql_robustness}, caption={An example of a SQL query for retrieving robustness values of the three experiments described in the~\nameref{ch:evaluation} chapter.}, float, language=SQL]{../figures/robustness_graffs.sql}


\section{Main pipeline}\label{sec:main_pipeline}

In this section I explain the steps a user needs to invoke to calculate robustness of metrics:
\begin{enumerate}[topsep=5pt,itemsep=0pt]
    \item \textbf{Obtain datasets}, i.e., use demo datasets downloadable by the \graffs tool or provide custom datasets
    \item \textbf{Define graph generator}, a way to generate new graphs from these datasets
    \item \textbf{Define an experiment} specifying a set of datasets, a graph generator to use, a set of metrics, and a set of robustness measures to assess stability of those metrics.
    \item \textbf{Run the experiment}, which encompasses the following:
    \begin{enumerate}[label=\alph*.]
        \item \textbf{Generate perturbed graphs} (according to \autoref{sec:perturbing_graphs}).
        \item \textbf{Calculate metric values} on the generated graphs, i.e., calculate a real number ($\mathbb{R}$) for each metric for each node in each generated graph (according to \autoref{sec:evaluating_metrics}).
        \item \textbf{Calculate robustness measures} for each metric, on collated perturbed graphs of each dataset (according to \autoref{sec:robustness_measures}).
    \end{enumerate}
\end{enumerate}

\input{figures/main_data_flow.tex}

Running the experiment is a computationally intensive task, which may, depending on the inputs and the environment, run in the order of magnitude of hours.
This is mainly due to metric evaluation (which certainly depends on the chosen metrics).
To mitigate this, I employed the following:
\begin{itemize}[topsep=5pt,itemsep=-2pt]
    \item Optimised the program for computation speed by applying performance engineering principles
    \item Allowed parallel computation
    \item ran experiments on a high-performance computing facility (from \autoref{sec:computing_cluster})
\end{itemize}

\autoref{fig:main_data_flow} illustrates the flow of data across the above-mentioned steps.

\subsection{Loading datasets}

\graffs supports loading graphs from files of two types.
In each case, the \texttt{GraphLoader} abstract class (\autoref{lst:code_graphloader}) is implemented.
\begin{description}[itemsep=\zerospace]
    \item \hyperref[sec:edge_files]{\textbf{Edge files}} -- text format, pair of adjacent nodes per line
    \item \hyperref[sec:rdata_files]{\textbf{\texttt{RData} files}} -- binary files with objects created in the R language~\cite{RCoreTeamLanguageEnvironmentStatistical2009}
\end{description}

Datasets are to be located in the `data' directory in the \graffs's working directory, each with a separate subfolder.
Datasets may additionally contain an \texttt{info.txt} file holding a human-readable description of the dataset.

\lstinputlisting[label={lst:code_graphloader}, caption={The \texttt{GraphLoader} abstract class, providing a contract for implementations to load graphs}, float=ht, language=Kotlin, firstline=6]{GraphLoader.kt}

\vspace*{-0.5mm}
\subsubsection*{Edge files}\label{sec:edge_files}

Listing a pair of nodes per line is a common pure format of storing network structure (without any metadata).
The set of nodes is implicitly defined by the set of all distinct node identifiers in the file.
\Cref{fig:edge_file_examples} shows a number of example edge files that are accepted by \graffs.

\input{figures/edge_file_examples.tex}

To make \graffs as versatile as possible, I implemented \texttt{FileSourceEdgeOptionalWeight} parser, extending GraphStream's \texttt{FileSourceEdge} simple LR parser, to allow for the following:
\begin{itemize}[topsep=5pt,itemsep=-2pt]
    \item Modify the grammar to correctly parse node IDs containing dots (\texttt{.}), such as \texttt{362663.ECP\_0001} (to allow datasets from the STRING database)
    \item Ignore comment lines starting with \texttt{\#} or \texttt{\%} characters
    \item Ignore headers, if present
    \item Recognise if the edges have weights provided
\end{itemize}

\subsubsection*{RData files}\label{sec:rdata_files}

The Paper provides the dataset files that were used in their research\footnote{Available at \url{http://opig.stats.ox.ac.uk/resources}}, in the \texttt{RData} format, a~binary file capturing an R language session, storing objects from the \texttt{igraph} R package.

To load such files, an option is to convert them manually into another \graffs-readable format, or let \graffs call R to do this conversion under the hood.
To make \graffs versatile and not require the R software environment to be installed on the client's computer, I employed the Renjin library\footnote{Rejin official website: \url{https://www.renjin.org/}}, a \enquote{JVM-based interpreter for the R programming language}.
Renjin can execute R code (as well as load RData files) within Java, without the need to install any additional platform.

When an RData file is to be loaded in \graffs, a Renjin engine starts under the hood and loads desired file.
If an object whose name ends with \texttt{.df}\footnotemark{} is found, it is converted into a GraphStream's \texttt{Graph} object.
\footnotetext{This is to follow convention from \url{https://github.com/lbozhilova/measuring_rank_robustness/blob/c39bfed/data_prep.R}}

\subsection{Generating graphs}

First, there is the \texttt{GraphProducer} interface, which has a single method:\\ \texttt{fun produce(sourceGraph: Graph, n: Int): List<Deferred<PerturbedGraph>>}.
It returns a list of \texttt{Deferred} objects from the \texttt{kotlinx.coroutines} library, so that the generation can be parallelised at a higher level in the program using \textsl{Kotlin coroutines}, as described in \autoref{sec:kotlin_coroutines}.
Each |GraphProducer| implementation provides a corresponding |GraphProducerFactory| object, defined as:

% @formatter:off
\begin{lstlisting}[language=Kotlin]
typealias GraphProducerFactory =
    (seed: Long, params: List<Number>, coroutineScope: CoroutineScope) -> GraphProducer
\end{lstlisting}
% @formatter:on

The \texttt{method} field of \texttt{GraphGenerator}s reference a \texttt{GraphProducer} to use, which can be either |threshold-linear| or |removing-edges| (as described in~\autoref{sec:perturbing_graphs}).
Implementations of the graph-producing methods are straightforward.
Each generated graph is in some way a memory-copied subset of the input graph, with some of the edges deleted.

\subsection{Metric robustness}

Metrics are classes (or rather, Kotlin |object|s) implementing the |Metric| interface, with a single method \texttt{fun evaluate(graph: Graph): MetricResult?}.
In the database, each |PerturbedGraph| entry stores a single serialised |Graph| object, whose nodes hold values for all metrics.
A metric's value at each node is stored in the node's attribute map.
An |Experiment| entry (those are created by the user) has a |Set<String>| field specifying which metrics to evaluate on each |PerturbedGraph|, and those metrics are again referenced by a |String| identifier.

Similar to other concepts, robustness measure classes implement the |RobustnessMeasure| interface, with a single method |suspend fun evaluate(metric: MetricInfo, graphCollection: GraphCollection, metadata: GraphCollectionMetadata): Double|.
|MetricInfo| uniquely identifies the |Metric| whose robustness to calculate.
|GraphCollection| is an entity corresponding to a dataset, with a list of |PerturbedGraph|s.
|GraphCollectionMetadata| provides cached\footnotemark{} access to intermediate results regarding the particular |GraphCollection| in the context of the current |Metric|, such as the \textsl{overall ranking} of the given metric over all the perturbed graphs (see \autoref{def:overall_ranking}).\footnotetext{cached, as the overall ranking is calculated only once for each \texttt{GraphCollection} and each \texttt{Metric}, and used for more robustness measures}
Each |RobustnessMeasure| returns a single |Double| value, which is the numerical result, calculated as described in \autoref{sec:robustness_measures}.
The |evaluate| function has the |suspend| modifier so that it can suspend and finish asynchronously within the context of Kotlin coroutines (see \autoref{sec:kotlin_coroutines}).

\subsubsection*{Resolving ties}

An important detail when using ranking vectors (e.g. for the overall ranking) is specifying how ties are resolved.
According to the definition of ranking vector (\cref{def:ranking_vector}), if we have $v_1, v_2 \in V$ with $M(v_1) = M(v_2)$ for some metric $M$, then $\preceq$ is a valid ranking relation with either $v_1 \preceq v_2$ or $v_2 \preceq v_1$.
In The Paper, such ties were resolved \enquote{at random, independently across different metrics and different thresholds}.
This may, however, lead to inappropriately lower robustness of metrics due to random oscillations of ranks of nodes, as well as a high degree of variability between results (especially for graphs with small number of nodes).

In \graffs, I assured that ties are resolved arbitrarily but deterministically and constantly across all metrics and thresholds, and even constantly across multiple perturbed graphs with the same nodes.
This follows the~\nameref{sec:reproducibility} principle set in~\nameref{sec:design-goals}.

\subsection{Visualising graphs \& producing figures}

This dissertation has charts, diagrams and visualisations of graphs from different sources.
\begin{itemize}[itemsep=-2pt,topsep=5pt]
    \item Plots that depend on computations within the \graffs program can be exported using the command line interface.
    \item Other figures relying on the code of \graffs (such as \cref{fig:simple_graph}) are generated by a custom code in the \texttt{figures} module.
    \item Other, independent plots, such as those in the~\nameref{ch:evaluation} chapter, are generated using Python and SQL, and that code is kept with the LaTeX sources of this dissertation.
\end{itemize}


\section{Parallelism using Kotlin coroutines}\label{sec:kotlin_coroutines}

Kotlin supports the concept of \textsl{suspending functions}, i.e., functions which can pause their execution (storing its \textsl{current continuation}) and return the control of the flow back to its caller, and later continue by resuming the continuation at the suspension point.
The context in which a suspending function suspends, is called coroutine.
Coroutines are a well-known control abstraction in programming languages~\cite{MouraRevisitingCoroutines2009} that can easily be implemented in higher-order languages using continuations\cite{HaynesContinuationsCoroutines1984}.
Asynchronous computations, delays, or IO operations can be programmed as suspending calls, i.e., functions that are allowed to \textsl{suspend} coroutine voluntarily, and return the control back to the caller, then continue whenever their \textsl{continuation} is resumed (or invoked).

Coroutines can be used even within a single thread, and thus allow a concurrent execution.
In the case of \graffs, however, we are interested in running computation in parallel, to utilise the power of multiple CPU cores.

Kotlin coroutines are \textsl{lightweight threads} -- spawning coroutines is fast and cheap~\cite{AriasFunctionalKotlinExtend2018}, as opposed to Java \texttt{Thread}s.
They have a similar life cycle and are executed within regular threads, although they are not bound to them~\cite{TorresLearningConcurrencyKotlin2018}.
Coroutines are also safer, e.g. a failure of one operation within a \textsl{coroutine scope} triggers cancellation of other coroutines in the scope.
Coroutines allow executing suspending functions.

Suspending functions in Kotlin, when compiled to Java bytecode, are regular functions accepting |Continuation<Int>| as one of the arguments.
The |async| function (or its alternatives) takes a suspending function, creates an asynchronous coroutine executing the function and immediately returns its future result (as |Deferred<T>|).
Then the |await| suspending function called on a |Deferred| object waits for the resulting value without blocking, and resumes when the deferred computation is complete.

\lstinputlisting[label={lst:kotlin_coroutines}, caption={A simplified example of creating an asynchronous task for evaluating a set of metrics on a single generated graph.}, float, language=Kotlin]{coroutines.kt}

Kotlin also provides other means to synchronise coroutines, such as using mutual exclusion or actors, when needed in a parallel execution.
Mutexes are used in \graffs to ensure synchronised access when persisting entities in the database.

%possibly: diagram similar to \url{https://s3.amazonaws.com/kukuruku-co/uploads/images/00/17/75/2016/10/09/126426.png}

The following parts of \graffs use parallel programming with coroutines:
\begin{itemize}[topsep=5pt]
    \item Generating graphs
    \item Evaluating metrics on graphs

    \Cref{lst:kotlin_coroutines} shows an example of creating this asynchronous task.
    It is heavily simplified; the real verion handles accessing the database, asynchronous deserialisation of \texttt{Graph}, logging, measuring time statistics, and preventing duplicate metric evaluation.
    Each |Graph| is treated individually without the need to synchronise across them, however, evaluation of metrics within one |Graph| is sequential as the GraphStream library does not support parallel access.
    \item Evaluating robustness, including calculating overall ranks, which must be calculated for each |GraphCollection| before any robustness measure is evaluated on it
    \item Generating various figures, for which an intermediate
\end{itemize}


\section{Command line interface}\label{sec:cli}

The \graffs tool is built as a program with rich and intuitive command-line interface.
The program is pre-packaged in a \texttt{.jar} file, built for Java 8 or higher, so running it involves running the shell command:
% @formatter:off
\begin{lstlisting}[language=bash,style=light]
java -jar graffs.jar [ARGS]
\end{lstlisting}
% @formatter:on

Clikt is a \enquote{multiplatform command line interface parsing for Kotlin}\footnote{Available at \url{https://github.com/ajalt/clikt}} library that I used for defining commands, their behaviour, description, and hierarchy, in a structured way.
The library utilises a number of features of the Kotlin language, such as delegated properties, extension functions, and default arguments, resulting in a cleaner code that uses the library.
With the help of Clikt, I generated scripts allowing automatic command and parameter completion within bash.

\input{figures_gen/cli_commands_hierarchy.tex}

The \graffs tool provides many commands (loading datasets, creating and accessing graph generators, creating and running experiments, and so on).
Commands are grouped based on their context and the objects that they operate on, and can further have sub-commands.
Those are together structured in a tree-like hierarchy.
The hierarchy is shown in \autoref{fig:cli_commands_hierarchy}.
\Cref{lst:graffs_cli_examples} shows an example output of some of the commands, with the printed help text.
Further, the \nameref{ch:evaluation} chapter also shows commands that were used to set up each experiment.
