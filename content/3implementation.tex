This chapter describes first the overall structure of the project, its technical requirements (which can and later be used as part of the evaluation) and then dives into the implementation of individual modules.

I refer to this project by its name \textsl{graffs}.
A working version is published on GitHub\footnote{\url{https://github.com/jjurm/graffs}} along with its source code.


\section{Overview}

The purpose of this project is to develop a methodology and tool, i.e. a framework, to help study graph metrics, and empirically analyse their robustness in particular.

\input{figures/overview_prop_diagram.tex}

\graffs is a command-line tool written in Kotlin\citeneeded that can load/store datasets of different formats, generate perturbed graphs, evaluate metrics, and calculate robustness values. \autoref{fig:overview_prop_diagram} is a diagram explaining the natural flow of the program, i.e. the \textsl{main pipeline} where we start with graph datasets and end up with deductions about each metric's


\section{Design goals}\label{sec:design-goals}

The following are technical requirements I set for the project.
Overall, I aim this tool to be reusable for similar projects, either by directly invoking the compiled binary, or by using it as a dependency, or by forking and extending it.

\todo{specify who is the target user of \graffs}

\subsection{Supported features}

\graffs supports the following features:

\begin{enumerate}
    \item Store, load input graphs in various formats, and represent them in a unified memory structure
    \item Run algorithms that compute metrics on graphs
    \item Generate graphs by applying perturbations to given input graphs
    \item Run experiments by evaluating metrics on generated graphs in a systematic manner
    \item Calculate robustness of metrics based on experiments
    \item Possibly, produce visual output from the results
\end{enumerate}

\subsection{Scalability}

According to The Paper~\cite{Bozhilova2019}, calculating natural connectivity for a single node for a graph with ~7000 nodes takes ~88 seconds on a standard computer.
In one of my toy examples, calculating average betweenness centrality of ~2500 nodes took ~8 minutes on my personal computer.
Thus, assuming computing a (computation-heavy) metric(s) on an input graph of average size 5000 nodes takes ~30 minutes, the pure computation time suggested by the~\nameref{ch:proposal} would take the following time on a standard personal computer (approximated in the order of magnitude)
\[(\sim 6\ \text{datasets}) \times (\sim 6\ \text{metrics}) \times (\sim 50\ \text{derived graphs}) \times (\sim 30\ \text{minutes}) \approx 38\ \text{days}\]

For this reason, one of the goals is to make the program efficient and runnable on a supercomputer, utilising the power of multi-core systems for parallel execution.

\subsection{Reproducibility}

It is important for all results in research to be reproducible.
By \textsl{reproducibility} of \graffs I mean the guarantee that one can exactly reproduce any results produced by the program.
I.e. when the program is run two times with the same input and hyper-parameters, it must produce the same output.
And by \textsl{the same output} I mean producing the same images, tables, numbers up to a bit-wise match.

This is a challenge in all the following areas:
\begin{itemize}
    \item \textbf{Stochastic processes}

    Methods based on stochastic processes or randomness must be reproducible.
    An example of a stochastic method are graph generators.

    These can be made reproducible done by generating all randomness starting off with a given seed for the generator.

    \item \textbf{Resolving ties}

    Methods that are not inherently stochastic but require pseudo-randomness to resolve ties must also be reproducible.
    An example is generating a visual layout for graphs such as in \autoref{fig:simple_graph}.
    This layout algorithm needs to resolve ties when starting with a graph where nodes have no position.

    Again, a solution is to base such flow on an input seed.

    \item \textbf{External factors}

    Unpredictability introduced by the operating system and other external factors must be accounted for, so that the program still produces the same results even if run on a different supported machine, in a different environment.
    This is more challenging in a concurrent environment when it must be made sure the produced output does not depend on any factors such as uncertainty and unpredictability of the OS's thread scheduler.

    These issues are resolved using a robust programming language and concurrency synchronisation approaches.
\end{itemize}

\subsection{Flexibility}

The program must be flexible enough, in particular the following:
\begin{enumerate}
    \item Usable on all widely used machines and operating systems
    \item Accepting input datasets (and any input parameters) in common formats
    \item As a library, it must provide modular access to individual parts of the program, so that it is easy to use \graffs as a dependency in future projects of a similar kind
\end{enumerate}


\section{Architecture}

In this section I explain major decisions about the program, such as the choice of the Kotlin language, and Git, Gradle tools.

\subsection{Kotlin language}

I used the programming language \textbf{Kotlin}, mainly for the following reasons.
It is by nature similar to Java and can be used together with other Java code in a single project.
Performance-wise, Kotlin is comparable to Java.
\begin{itemize}
    \item Concise, reducing the amount of boilerplate code
    \item Safer, preventing a significant number of errors
    \item IDE-friendly, allowing the IDE to help with software engineering
    \item Employs functional programming paradigms\cite{Bonev}
    \item Compiles to Java byte code and so preserves other important benefits of Java: Object-Oriented, multi-threaded, platform-independent, secure and easily extensible.
\end{itemize}

Using Kotlin in the project still allows including any libraries written in Java, as Kotlin compiler compiles \texttt{.kt} files to Java-bytecode \texttt{.class} files.
Kotlin has most of its concepts and features adopted from Java, such as classes, polymorphism, inheritance, so these concepts I will freely use in this work.

Kotlin has a number of advanced features such as improved type safety and reduced boilerplate code\cite{JemerovKotlinAction2017}.
There is one notable feature of Kotlin: properties.
Properties are an abstraction of fields and getters/setters in classes and help decouple the implementation of the class from its interface even more.
In UML diagrams in this work , instead of presenting fields and methods of each class, listed are methods and properties (in this order).

\todo{diagram of Kotlin, Gradle, Git, server}

\subsection{Version Control System}

The source code of the \graffs tool is stored in a Git repository, which keeps track of all code changes and allows understanding how code changed over time as well as restoring previous versions.

The repository can be found at \url{https://github.com/jjurm/graffs}.

\subsection{Build automation}

The project uses Gradle\citeneeded for project management.
Split into different modules, Gradle also helps to keep the structure well defined and manages builds of each module separately (which is called a multi-project build in Gradle).

Project configuration rules are set up using the \texttt{build.gradle} files (one in the root directory, then one within each module) with a number of plugins to facilitate the following and more:
\begin{enumerate}
    \item Defines the structure of the project, such as the directories for each module, and source and build directories of each
    \item Automates the process of compiling the code, running tests and producing deployable \texttt{jar}s
    \item Manages and automatically downloads dependencies
    \item Helps with version numbering
    \item Generates HTML API documentation for Kotlin and Java classes
\end{enumerate}


\todo{maybe put all subsections below somewhere at the end of Implementation}

\subsection{Computing cluster}\label{sec:computing_cluster}

I used a remote high-performance computing facility provided by the Systems Research Group (\url{https://www.cl.cam.ac.uk/research/srg/}), sponsored by Dr Andrew Moore (\url{andrew.moore@cl.cam.ac.uk}).

In particular, I worked with the server \texttt{rio.cl.cam.ac.uk} with \todo{include computing characteristics}.

\subsubsection{Setting up the cluster}

\subsection{Project modules}

The project is structured in the following modules, using multi-project builds in Gradle:
\begin{itemize}
    \item \texttt{core} - APIs for structures, metrics, generators, etc., as well as the core data model for storing data in the database
    \item \texttt{storage} - accessing and loading graphs/datasets stored in the filesystem
    \item \texttt{generators} - graph generators
    \item \texttt{metrics} - graph metrics
    \item \texttt{robustness} - robustness measures
    \item \texttt{cli} - code for command-line interface
\end{itemize}

\todo{diagram of code structure + modules + packages}


\section{Data model}

The \graffs tool uses a number of libraries to represent graphs in memory, define a data model, and store data in a relational database.
This section explains how the data of the program is persisted.

The following entities highligh the main concept, and there are some more entities stored in the database.
\begin{itemize}[topsep=5pt]
    \item \textsl{Graph} storing its nodes, edges (and their attributes)
    \item \textsl{Graph generator} is an object capable of producing a number of graphs, given a dataset.
    \item \textsl{Experiment} is a description of a computational task involving:
    \begin{enumerate}[topsep=5pt]
        \item generating graphs from given input datasets using a given graph generator
        \item evaluating a given set of metrics on all generated graphs
        \item evaluate given robustness measures on all metrics over all datasets of the experiment
    \end{enumerate}
\end{itemize}

A user can create and manage \textsl{graph generators} and \textsl{experiments} using the command line interface (see \autoref{sec:cli}).

\subsection{Using GraphStream}

Graphs in memory are stored and manipulated by the GraphStream library\cite{DutotGraphStreamToolBridging2007}, a \enquote{Java library for the modeling and analysis of dynamic graphs. You can generate, import, export, measure, layout and visualize them}

\input{figures_gen/graphstream_diagram.tex}

The library is based around the \texttt{Graph} interface, which provides access to \texttt{Node}s and \texttt{Edge}s of each graph.
The most relevant interfaces are illustrated in \autoref{fig:graphstream_diagram} (heavily simplified).
The interfaces also provide methods for changing graphs (adding/removing nodes, edges, attributes, etc.).
In practice the library's interfaces contain many more links and methods (e.g. for handling directed graphs) that are not relevant in the context of this project.

For evaluation of robustness, we need to be able to compare (ranks of) values of a specific node between two or more generated graphs, therefore we need to be able to preserve mapping of nodes of a generated graph to nodes in the original dataset (see definition~\ref{def:graph_matching}).
Note that all \texttt{Element}s (i.e. \texttt{Node}s, \texttt{Edge}s and even \texttt{Graph}s) have an \texttt{id} field which will be used for matching nodes between base and perturbed graphs.
This makes evaluation of robustness more straightforward as ranks of nodes in respect to a given metric can be compared across multiple graphs generated from the same base graph.

The GraphStream library also provides a solid base for numerous features in this project such as loading graph files, storing them in various formats, and visualisations.
The GraphStream library allows working with dynamic graphs (changing over time), my project only uses static graphs.
The library also contains an implementation of some graph metrics.

\parspace

The \texttt{Graph} object from the GraphStream library stores (references) all nodes and edges, along with attributes.
In my project, if a metric has been evaluated on a graph, the metric's value for each node is stored as the \texttt{Node}'s attribute, all contained within the \texttt{Graph} object.
The attribute key is given my the metric name.

\subsection{Relational model}

Data of the \graffs tool such as generated graphs, evaluated metrics, robustness measure results as well as any user-defined hyper-parameters are stored in a relational database system.
The Java Persistence API (JPA)~\cite{BiswasJavaPersistenceAPI2016} provides an abstraction for accessing relational data from Java, Hibernate~\cite{ElliottHibernateDeveloperNotebook2004,BauerJavaPersistenceHibernate2015} is a framework that implements the inteface.
I used specifically the H2 database engine\cite{MuellerH2DatabaseEngine2006} as the underlying storage for Hibernate.
This abstraction is later illustrated in \autoref{fig:orm_kotlin_h2_diagram}.

\input{figures_gen/data_model_diagram.tex}

\autoref{fig:data_model_diagram} shows the entities that the program persists in the database, as explained below.
Named entities (\texttt{Experiment}, \texttt{GraphGenerator}) are those that the user creates and later refers to with their name.
\begin{description}
    \item[\texttt{PerturbedGraph}]
    stores a serialised version of the \texttt{Graph} object, inclucing all so-far evaluated metric values of nodes, and possibly a weight associated with each edge (for scored networks).
    It also stores metadata, such as a hash which distinguishes the graph from other graphs beloging to the same \texttt{GraphCollection} coming from the same graph generator.
    For example in the case graphs are generated by \hyperref[sec:randomly_removing_edges]{randomly removing edges}, the starting seed value of the generator of the particular graph is used as its hash.

    \item[\texttt{GraphCollection}]
    represents an (ordered) collection of \texttt{PerturbedGraph}s.
    It also keeps track of which datasets the graphs were generated from.

    \item[\texttt{GraphGenerator}]
    is an entity that stores user-defined rules for generating graphs from a dataset.
    Once a graph generator is defined and created, it can universally be used on multiple input datasets.
    The parameters of a generator include the name of the method to use (such as \texttt{linear-thresholding}), number of perturbed graphs to generate from each input dataset, seed, and any additional numeric parameters specific to each generator.

    \item[\texttt{Experiment}]
    an entity that encapsulates a concept of evaluating multiple \textsl{robustness measures} of multiple \textsl{metrics} on multiple \textsl{datasets}, using a specific graph generator.
    With an existing generator, the user defines an experiment, which can then be evaluated (see \autoref{sec:main_pipeline}).

    \item[\texttt{Robustness}]
    an entity that stores a single result of evaluating a \textsl{robustness measure} of a \textsl{metric}, on a set of perturbed graphs originating from a certain \textsl{dataset}.
    These robustness values each belong to its parent \texttt{Experiment}.
    Note (\autoref{fig:data_model_diagram}) that the four fields \texttt{experiment}, \texttt{dataset}, \texttt{metric}, \texttt{robustnessMeasure} together form the primary key.
\end{description}

\subsection{Java Persistence API}

The Java Persistence API\cite{BiswasJavaPersistenceAPI2016} (JPA) is an API specification for management of relational data in Java.
It describes ways in Java to specify schemas of relational databases and an interface to manage and access data of a relational model (i.e. entities in tables, relations, first-order logic).
\textsl{Persistence} is an abstract term referring to accessing, managing, and storing entities.

The Hibernate framework~\cite{ElliottHibernateDeveloperNotebook2004,BauerJavaPersistenceHibernate2015}, an object-relational mapping tool, provides a concrete implementation of JPA.
I use Hibernate as the intermediate layer between the \texttt{core} module of \graffs and the underlying H2 database that is completely abstracted away from the \graffs code (\autoref{fig:orm_kotlin_h2_diagram}).

\input{figures/orm_kotlin_h2_diagram.tex}

\input{figures_gen/data_model_classes_diagram.tex}

Further, \autoref{fig:data_model_classes_diagram} shows the underlying \textsl{entity classes} written in Kotlin that have the following functions:
\begin{itemize}[topsep=5pt,label=$\bm{\rightarrow}$]
    \item \textbf{Data model definition}, seen in \autoref{fig:data_model_diagram}

    JPA provides a number of Java annotations to define entities (\texttt{@Entity}), their fields (\texttt{@Column}), constraints such as foreign key constraint (\texttt{@OneToMany}, \texttt{@ManyToOne} and others), and metadata such as rules for fetching data from database (e.g. \texttt{@Basic(fetch = FetchType.LAZY)} for lazy fetching of a field).
    Data model is inferred from these annotations that are processed during compile time

    Hibernate then abstracts away also operations such as creating and updating the database schema, which are configured in the \texttt{hibernate.cfg.xml} file (for example it creates a database from scratch if it doesn't exist).

    See \autoref{lst:kotlin_experiment_entity} for an example entity class using the JPA annotations.

    \item \textbf{Metamodel generation}

    Taking the annotated entity classes during compile time, Hibernate generates a \textsl{metamodel} class for each entity to allow writing type-safe queries.
    For example, the \texttt{Experiment\_} class is generated from the \texttt{Experiment} entity, with corresponding fields (such as \texttt{Experiment\_.name} of type \texttt{SingularAttribute<NamedEntity, String>}, or \texttt{Experiment\_.generator} of type \texttt{SingularAttribute<Experiment, GraphGenerator>}).

    See \autoref{lst:jpa_typed_query} for an illustration how generated metamodel classes aid in writing type-safe queries.

    \lstinputlisting[label={lst:jpa_typed_query}, linerange=jpa_typed_query0-jpa_typed_query1, caption={A toy example of using typed JPA queries. Note the \texttt{Experiment\_} metamodel class generated by Hibernate with (meta)fields corresponding to entities' fields}, float, language=Kotlin]{listings.kts}

    \item \textbf{Object-relational mapping}

    The classes (\autoref{fig:data_model_classes_diagram}) themselves carry the data managed by Hibernate.
    This means that other modules such as \texttt{generators} and \texttt{robustness} can use entity classes, while they are also managed by Hibernate.

    Hibernate is responsible for loading data into entity objects as well as persisting such objects.
    See \autoref{lst:hibernate_load_entity} for an illustration.

    \lstinputlisting[label={lst:hibernate_load_entity}, linerange=hibernate_load_entity0-hibernate_load_entity1, caption={A toy example of Hibernate loading and persisting an \texttt{Experiment} object.}, float, language=Kotlin]{listings.kts}
\end{itemize}


\autoref{lst:kotlin_experiment_entity} presents a full code listing of the \texttt{Experiment} entity.

\lstinputlisting[label={lst:kotlin_experiment_entity}, caption={The \texttt{Experiment} class written in Kotlin. Note especially the annotations which are enough to define the JPA data model.}, float, firstline=11,language=Kotlin]{Experiment.kt}

\subsection{H2 Database}

I employed the H2 relational database~\cite{MuellerH2DatabaseEngine2006} (based on SQL language) for storing entities, considering the following:
\begin{itemize}[topsep=5pt]
    \item Very fast; small footprint
    \item Easily embeddable, as it implements the JDBC API (Java Database Connectivity API, used by Hibernate too)
    \item Supports in-memory databases (good for testing)
    \item Written purely in Java, so can be bundled in \graffs and thus requires no other database installation in the client OS
\end{itemize}

\subsubsection*{Storing graphs in database}

Considering exporting graphs to various formats, I chose to store them purely as serialised \texttt{Graph} objects, including: \texttt{Node}s, \texttt{Edges}, and their attributes.


\section{Main pipeline}\label{sec:main_pipeline}

In this section I explain the steps of \graffs involved in calculating robustness measures of metrics.
In summary, the normal steps are:
\begin{enumerate}
    \item \textbf{Obtain datasets}, for example use demo datasets downloadable by the \graffs tool or provide any custom datasets
    \item \textbf{Define graph generator}, i.e. a way to generate new graphs from these datasets
    \item \textbf{Define an experiment} specifying a set of datasets to start with, a graph generator to use, a set of metrics to asses, and a set of robustness measures to evaluate stability of those metrics.
    \item \textbf{Run the experiment} computation (a computationally intensive task, which may, depending on the inputs and the environment, run in the order of magnitude of hours)

    This encompasses the following:
    \begin{enumerate}[label=\alph*.]
        \item \textbf{Generate perturbed graphs} (according to \autoref{sec:perturbing_graphs})
        \item \textbf{Calculate metric values} on the generated graphs, i.e. calculating a real number ($\mathbb{R}$) for each metric for each node in each generated graph (according to \autoref{sec:evaluating_metrics}).
        \item \textbf{Calculate robustness measures} for each metric, on collated perturbed graphs of each dataset
    \end{enumerate}
\end{enumerate}

All results, namely generated graphs with nodes including calculated values of each metric, and robustness measure results, are then stored in the database.

\input{figures/main_data_flow.tex}

Running the experiment is a computationally intensive task, mainly due to metric evaluation (which certainly depends on the chosen metrics).
To mitigate this, I employed the following:
\begin{itemize}[topsep=5pt]
    \item Optimised the program for computation speed by applying performance engineering principles \todo{elaborate}
    \item Allow parallel computation
    \item I ran extensive experiments on a high-performance computing facility (from \autoref{sec:computing_cluster})
\end{itemize}

\autoref{fig:main_data_flow} illustrates the flow of data across the above-mentioned steps.

\subsection{Loading datasets}

Datasets are located, and are expected to be located, in the `data' directory in the \graffs's working directory (i.e. where the program is run).
Each sub-directory represents a dataset identified by the sub-directory's name.
\graffs supports loading files of two different types of files:

\begin{description}
    \item \hyperref[sec:edge_files]{\textbf{Edge files}} -- text format with each line containing a pair of adjacent nodes
    \item \hyperref[sec:rdata_files]{\textbf{\texttt{RData} files}} -- binary files that store objects created in the R language~\cite{RCoreTeamLanguageEnvironmentStatistical2009}
\end{description}

\input{figures_gen/data_dir_structure.tex}

In each dataset's directory, the program looks for a file that ends with either \texttt{.RData} or \texttt{.txt}, in this order.
Datasets may additionally contain an \texttt{info.txt} file holding a human-readable description of the dataset (that is printed in the command-line interface when the dataset is loaded).
\autoref{fig:data_dir_structure} shows an example directory structure.

The \texttt{GraphLoader} abstract class (\autoref{lst:code_graphloader}) is to be implemented in both cases.
\lstinputlisting[label={lst:code_graphloader}, caption={The \texttt{GraphLoader} abstract class, providing a contract for implementations to load graphs}, float=h, language=Kotlin, firstline=6]{GraphLoader.kt}

\subsubsection{Edge files}\label{sec:edge_files}

A common pure format of storing network structure (without any metadata) is to list the edges line by line, with each line containing unique identifiers of two nodes.
The set of nodes is then implicitly defined by the set of all distinct node identifiers present in the file.

However, different dataset sources (\autoref{sec:datasets}) provide \texttt{.txt} edge files with the content in different formats.
Some edge files contain a header, \texttt{#comments}, or a third number in each line representing the edge's weight (for \hyperref[sec:scored_networks]{scored networks}).
\autoref{fig:edge_file_examples} shows a number of example edge files that should be accepted by \graffs.

\input{figures/edge_file_examples.tex}

One way would be to distinguish different formats by the file extension, or with additional meta information.
However, one of the goals for \graffs is to make it as versatile as possible, thus not requiring the user to manually specify a dataset's type or source is welcome.

Hence, I implemented \texttt{FileSourceEdgeOptionalWeight} parser, extending GraphStream's \texttt{FileSourceEdge} simple LR parser, to allow the following:
\begin{itemize}[topsep=5pt]
    \item Modify the grammar to correctly parse node IDs containing dots (\texttt{.}), such as \texttt{362663.ECP\_0001} (to allow datasets from the STRING database)
    \item Ignore comment lines starting with the \texttt{#} character
    \item Ignore headers starting with the string \texttt{protein1} (included in graph files downloaded from the STRING database)
    \item Recognise if a line has 3 tokens instead of 2, in which case parse and store it as the weight of the edge
\end{itemize}

\subsubsection{RData files}\label{sec:rdata_files}

The Paper\cite{Bozhilova2019} provides the dataset files that were used in their research\footnote{Available at \url{http://opig.stats.ox.ac.uk/resources}}, in the \texttt{RData} format, a binary file capturing a session of the R language, storing objects from the \texttt{igraph} R package.

To load such files, an option is to convert them manually into another \graffs-readable format, or let \graffs call R to do this conversion under the hood.
To make \graffs as versatile as possible and not require the R software environment to be installed on the client's computer, I employed the Renjin library\footnote{Rejin official website: \url{https://www.renjin.org/}}, a \enquote{JVM-based interpreter for the R programming language}.
Renjin can execute R code (as well as load RData files) within Java, without the need to install any additional platform.

When an RData file is to be loaded in \graffs, a Renjin engine starts under the hood and loads desired file.
If an object ending with \texttt{.df}\footnotemark is found, it is converted into a GraphStream's \texttt{Graph} object.
\footnotetext{This is to follow convention from \url{https://github.com/lbozhilova/measuring_rank_robustness/blob/c39bfed/data_prep.R}}

\subsection{Generating graphs}

\todo{work in progress}

For evaluating robustness of graph metrics according to the model described in the~\nameref{ch:proposal}, we need to evaluate graph metrics on a number of similar graphs - graphs that all describe the same facts from the real world.
We need to be able compare value of a metric between different graphs that \textit{share the same source} or are \textit{of the same foundation}.

One specific example may be a graph constructed from social network, such as Facebook.
Let $G_0$ be a dataset constructed from people and their mutual friendships at Facebook, and let $G_1$ be a graph constructed from people, with the set of edges including only stronger friendships. $G_0$ and $G_1$ have the same nodes, but edges of $G_1$ is a subset of edges of $G_0$.
Now, $G_0, G_1$ are different but describe the same structure of people in the world, i.e.\ convey the same meaning.

\textbf{Preserving node identities} For two such graphs, we can define subsets $V_{c0}$ and $V_{c1}$ of nodes of the respective graphs, such that there exists a bijection $V_{c0} \leftrightarrow V_{c1}$.
These nodes will have important meaning for definition of the robustness function, because the pairs of corresponding nodes describe the same entities of the real world.
Thus, we can observe how a graph metric behaves for a particular node in different graphs.

\subsubsection{Random edge deletion}

\subsubsection{Thresholding edges of protein graphs}

\subsection{Evaluating metrics}

\subsection{Metric robustness}

\subsection{Visualising graphs}


\section{Parallelism using Kotlin coroutines}


\section{Command line interface}\label{sec:cli}

\todo{hierarchical diagram of nested command groups and their options}


\section{Testing}

\subsection{Unit testing}
Using JUnit 5

\subsection{Continuous Integration}
CircleCI


\section{Documentation}

The Kotlin code is documented using the \texttt{KDoc} language and an HTML documentation is generated using the \texttt{Dokka} tool (similar to \texttt{JavaDoc}).\citeneeded

I wrote the documentation in code most thoroughly for public classes, and their members that need clarification on their behaviour or usage.



